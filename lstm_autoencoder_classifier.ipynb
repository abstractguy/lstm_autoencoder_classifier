{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "lstm_autoencoder_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abstractguy/lstm_autoencoder_classifier/blob/master/lstm_autoencoder_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZOZmPmIyyxr",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Autoencoder for Rare Event Binary Classification\n",
        "\n",
        "This is a continuation of the regular autoencoder for rare event classification presented in\n",
        "https://towardsdatascience.com/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098\n",
        "and code present in\n",
        "https://github.com/cran2367/autoencoder_classifier/blob/master/autoencoder_classifier.ipynb\n",
        "Here we will show an implementation of building a binary classifier using LSTM Autoencoders. \n",
        "Similar to the previous post, the purpose is to show the implementation steps. The Autoencoder tuning for performance improvement can be done.\n",
        "\n",
        "LSTM requires closer attention to preparing the data. Here we have all the steps, and few tests to validate the data preparation.\n",
        "\n",
        "The dataset used here is taken from here,\n",
        "\n",
        "**Dataset: Rare Event Classification in Multivariate Time Series** https://arxiv.org/abs/1809.10717 (please cite this article, if using the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cled-IrXTDrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hardcode non-shared parameters.\n",
        "AI = 'LSTM_autoencoder'\n",
        "main_ticker = 'AAPL'\n",
        "tickers_list = ['INTC', 'AAPL', 'NVDA', 'CSCO', 'AMD', 'AMZN', 'GOOG', 'MSFT', 'S', 'BAC', 'XLNX', 'WFC', '^DJI', '^GSPC', '^NYA', '^IXIC']\n",
        "unit_scaler = 3.4\n",
        "length = 9\n",
        "random_seed = 123 # Used to help randomly select the data points.\n",
        "test_size = 0.2\n",
        "gain = 0.07"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr24TOdmTKAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hardcode shared parameters.\n",
        "if AI == 'LSTM':\n",
        "    trends = True\n",
        "    batch_size = 64\n",
        "    epochs = 200\n",
        "    learning_rate = 0.0000013\n",
        "\n",
        "elif AI == 'LSTM_autoencoder':\n",
        "    trends = False\n",
        "    batch_size = 32\n",
        "    epochs = 200\n",
        "    learning_rate = 0.0002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIaFe10aEU8k",
        "colab_type": "code",
        "outputId": "5764be39-6817-410f-d7d4-74f88d7d5385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "!pip install pytrends"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.6/dist-packages (4.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytrends) (0.24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytrends) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas->pytrends) (1.16.4)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pytrends) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->pytrends) (2.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (2019.6.16)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->pytrends) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJuuvLMCEVuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from collections import OrderedDict\n",
        "from math import sqrt\n",
        "from os import chdir\n",
        "from os.path import exists\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from numpy.random import seed\n",
        "from numpy import append, array, concatenate, count_nonzero, empty, empty_like, expand_dims, mean, nan, power, var, where, zeros\n",
        "from pandas import concat, DataFrame, date_range, read_csv, Series\n",
        "from pandas_datareader.data import DataReader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import auc, classification_report, confusion_matrix, f1_score, mean_squared_error, precision_recall_curve, precision_recall_fscore_support, recall_score, roc_curve\n",
        "from tensorflow import set_random_seed\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import load_model, Model, Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, RepeatVector, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, TerminateOnNaN\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras import optimizers, Sequential\n",
        "from google.colab.drive import mount\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "from pytrends.request import TrendReq\n",
        "from pylab import rcParams\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9PfXIoazLPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa323ec2-2d9e-4d6c-f61d-af91166b945a"
      },
      "source": [
        "path = '/content/gdrive/'\n",
        "mount(path)\n",
        "path = path + 'My Drive/LSTM_autoencoder/'\n",
        "chdir(path)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koUXcR8eCYt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sign = lambda x: (1, -1)[x < 0]\n",
        "\n",
        "def curve_shift(dataset, shift_by):\n",
        "    '''\n",
        "    This function will shift the binary labels in a dataframe.\n",
        "    The curve shift will be with respect to the 1s. \n",
        "    For example, if shift is -2, the following process\n",
        "    will happen: if row n is labeled as 1, then\n",
        "    - Make row (n+shift_by):(n+shift_by-1) = 1.\n",
        "    - Remove row n.\n",
        "    i.e. the labels will be shifted up to 2 rows up.\n",
        "    \n",
        "    Inputs:\n",
        "    dataset  A pandas dataframe with a binary labeled column. \n",
        "             This labeled column should be named as 'y'.\n",
        "    shift_by An integer denoting the number of rows to shift.\n",
        "    \n",
        "    Output\n",
        "    dataset  A dataframe with the binary labels shifted by shift.\n",
        "    '''\n",
        "\n",
        "    vector = dataset['y'].copy()\n",
        "    for s in range(abs(shift_by)):\n",
        "        tmp = vector.shift(sign(shift_by))\n",
        "        tmp = tmp.fillna(0)\n",
        "        vector += tmp\n",
        "    labelcol = 'y'\n",
        "    # Add vector to the DataFrame.\n",
        "    dataset.insert(loc=0, column=labelcol+'tmp', value=vector)\n",
        "    # Remove the rows with labelcol == 1.\n",
        "    dataset = dataset.drop(dataset[dataset[labelcol] == 1].index)\n",
        "    # Drop labelcol and rename the tmp col as labelcol.\n",
        "    dataset = dataset.drop(labelcol, axis=1)\n",
        "    dataset = dataset.rename(columns={labelcol+'tmp': labelcol})\n",
        "    # Make the labelcol binary.\n",
        "    dataset.loc[dataset[labelcol] > 0, labelcol] = 1\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def add_ticker_to_tickers(symbol, ticker, tickers):\n",
        "    minimum_date = ticker.index.min()\n",
        "    overall_minimum_date = tickers.index.min()\n",
        "    if minimum_date > overall_minimum_date:\n",
        "        start = overall_minimum_date\n",
        "        tickers = tickers[tickers.index >= minimum_date]\n",
        "\n",
        "    tickers[symbol][tickers.index.isin(ticker.index)] = ticker\n",
        "\n",
        "    return tickers\n",
        "\n",
        "def get_stock(symbols, path=path, trends=True):\n",
        "    csv_path_X = path + 'dataset_X.csv'\n",
        "    csv_path_y = path + 'dataset_y.csv'\n",
        "\n",
        "    if exists(csv_path_X) and exists(csv_path_y):\n",
        "        tickers_X = read_csv(csv_path_X)\n",
        "        tickers_y = read_csv(csv_path_y)\n",
        "    else:\n",
        "        start = datetime(1970, 1, 1)\n",
        "        end = datetime.now()\n",
        "        dates = date_range(start=start, end=end)\n",
        "        columns = concatenate([symbols, [(symbol + '_trend') for symbol in symbols]]) if trends else symbols\n",
        "        tickers_X = DataFrame(nan, index=dates, columns=columns)\n",
        "        tickers_y = DataFrame(nan, index=dates, columns=columns)\n",
        "        tickers_X.index.name = 'Date'\n",
        "        tickers_y.index.name = 'Date'\n",
        "        if trends:\n",
        "            pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "        for symbol in tqdm(symbols, unit='symbol'):\n",
        "            try:\n",
        "                ticker = DataReader(symbol, 'yahoo', start=start, end=end)\n",
        "                ticker_Open = ticker.Open\n",
        "                ticker_Close = ticker.Close\n",
        "                ticker_X = (ticker_Close - ticker_Open).dropna()\n",
        "                ticker_y = (ticker_Close / ticker_Open).dropna()\n",
        "                tickers_X = add_ticker_to_tickers(symbol, ticker_X, tickers_X)\n",
        "                tickers_y = add_ticker_to_tickers(symbol, ticker_y, tickers_y)\n",
        "                if trends:\n",
        "                    pytrends.build_payload([symbol], timeframe='today 5-y')\n",
        "                    ticker = pytrends.interest_over_time()[symbol]\n",
        "                    tickers_X = add_ticker_to_tickers(symbol + '_trend', ticker.copy(), tickers_X)\n",
        "                    tickers_y = add_ticker_to_tickers(symbol + '_trend', ticker.copy(), tickers_y)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        tickers_X = tickers_X.fillna(method='backfill').dropna()\n",
        "        tickers_y = tickers_y.fillna(method='backfill').dropna()\n",
        "\n",
        "        tickers_X.to_csv(csv_path_X)\n",
        "        tickers_y.to_csv(csv_path_y)\n",
        "\n",
        "    return tickers_X, tickers_y\n",
        "\n",
        "def delta_time_series(data):\n",
        "    return data[1:] - data[:-1]\n",
        "\n",
        "def plot_dataset(dataset):\n",
        "    plt.plot(dataset)\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Derivatives')\n",
        "    plt.show()\n",
        "\n",
        "def get_y_from_generator(generator):\n",
        "    \"\"\"Get all targets y from a TimeseriesGenerator instance.\"\"\"\n",
        "    y = None\n",
        "\n",
        "    for i in range(len(generator)):\n",
        "        batch_y = generator[i][1]\n",
        "\n",
        "        if y is None:\n",
        "            y = batch_y\n",
        "        else:\n",
        "            y = append(y, batch_y)\n",
        "\n",
        "    y = y.reshape((-1, 1))\n",
        "    print(y.shape)\n",
        "    return y\n",
        "\n",
        "def binary_accuracy(a, b, name='training'):\n",
        "    \"\"\"Helper function to compute the match score of two binary numpy arrays.\"\"\"\n",
        "    a = a[:,0] > 0\n",
        "    b = b[:,0] > 0\n",
        "    assert len(a) == len(b)\n",
        "    print('Binary accuracy (' + name + ' data):', (a == b).sum() / len(a))\n",
        "\n",
        "def compute_units(X_train):\n",
        "    sample_size, feature_size = X_train.shape\n",
        "    units = int(((sample_size / unit_scaler) / length) - feature_size)\n",
        "    print('Units:', units)\n",
        "    return units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ1ce3K4Cfbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tickers_list = [main_ticker] + tickers_list\n",
        "tickers_list = list(OrderedDict((ticker, True) for ticker in tickers_list).keys())\n",
        "dataset_csv_X, dataset_csv_y = get_stock(tickers_list, trends=trends)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVaCgqfUUYkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset_csv_y.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ij_2mhC9hD",
        "colab_type": "code",
        "outputId": "b91b474c-d3ca-4dae-8ee7-cced6fcace3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Ground truth.\n",
        "dataset['y'] = (dataset[main_ticker] > (1.0 + (gain / 2.0))).astype(int)\n",
        "print('Percentage of ones (keep less than 5%):', count_nonzero(dataset.y) / dataset.y.size)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of ones (keep less than 5%): 0.026330224904004388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aMNeaIpn48n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7hifspdE8D9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed(7)\n",
        "set_random_seed(11)\n",
        "rcParams['figure.figsize'] = 8, 6\n",
        "LABELS = ['Normal', 'Break']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO_8_S2myyyL",
        "colab_type": "code",
        "outputId": "c497f5f7-2750-43dc-e78b-42e2d116378c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "print('Before shifting') # Positive labeled rows before shifting.\n",
        "one_indexes = dataset.index[dataset['y'] == 1]\n",
        "display(dataset.iloc[(one_indexes[0]-3):(one_indexes[0]+2), 0:5].head(n=5))\n",
        "dataset = curve_shift(dataset, shift_by=-1)\n",
        "\n",
        "print('After shifting') # Validating if the shift happened correctly.\n",
        "display(dataset.iloc[(one_indexes[0]-4):(one_indexes[0]+1), 0:5].head(n=5))  "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before shifting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Date</th>\n",
              "      <th>AAPL</th>\n",
              "      <th>INTC</th>\n",
              "      <th>NVDA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2004-08-22</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "      <td>1.005578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2004-08-23</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "      <td>1.005578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>2004-08-24</td>\n",
              "      <td>1.022073</td>\n",
              "      <td>0.977006</td>\n",
              "      <td>0.949219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>2004-08-25</td>\n",
              "      <td>1.037025</td>\n",
              "      <td>1.011987</td>\n",
              "      <td>1.020129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>2004-08-26</td>\n",
              "      <td>1.049031</td>\n",
              "      <td>0.995883</td>\n",
              "      <td>1.003953</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index        Date      AAPL      INTC      NVDA\n",
              "3      3  2004-08-22  1.007129  1.003208  1.005578\n",
              "4      4  2004-08-23  1.007129  1.003208  1.005578\n",
              "5      5  2004-08-24  1.022073  0.977006  0.949219\n",
              "6      6  2004-08-25  1.037025  1.011987  1.020129\n",
              "7      7  2004-08-26  1.049031  0.995883  1.003953"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "After shifting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>index</th>\n",
              "      <th>Date</th>\n",
              "      <th>AAPL</th>\n",
              "      <th>INTC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2004-08-21</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2004-08-22</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2004-08-23</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "      <td>2004-08-24</td>\n",
              "      <td>1.022073</td>\n",
              "      <td>0.977006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2004-08-27</td>\n",
              "      <td>0.990484</td>\n",
              "      <td>1.007319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     y  index        Date      AAPL      INTC\n",
              "2  0.0      2  2004-08-21  1.007129  1.003208\n",
              "3  0.0      3  2004-08-22  1.007129  1.003208\n",
              "4  0.0      4  2004-08-23  1.007129  1.003208\n",
              "5  1.0      5  2004-08-24  1.022073  0.977006\n",
              "8  0.0      8  2004-08-27  0.990484  1.007319"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc41Mo3qSRXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.drop(columns=['Date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rosttrXFyyyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the DataFrame to a numpy array.\n",
        "input_X = dataset.loc[:,dataset.columns != 'y'].values\n",
        "input_y = dataset['y'].values\n",
        "\n",
        "n_features = input_X.shape[1] # Number of features."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCMHcaFmyyyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def temporalize(X, y, lookback):\n",
        "    output_X = []\n",
        "    output_y = []\n",
        "    for i in range(len(X)-lookback-1):\n",
        "        t = []\n",
        "        for j in range(1,lookback+1):\n",
        "            # Gather past records upto the lookback period.\n",
        "            t.append(X[[(i+j+1)], :])\n",
        "        output_X.append(t)\n",
        "        output_y.append(y[i+lookback+1])\n",
        "    return output_X, output_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgLp2Oyiyyyp",
        "colab_type": "code",
        "outputId": "f5f7f013-9375-4640-f1d6-be4453a4a8ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "# Test: The 3D tensors (arrays) for LSTM are forming correctly.\n",
        "print('First instance of y = 1 in the original data')\n",
        "display(dataset.iloc[(where(array(input_y) == 1)[0][0]-5):(where(array(input_y) == 1)[0][0]+1), ])\n",
        "\n",
        "# Equivalent to 5 days of past data.\n",
        "lookback = 5\n",
        "# Temporalize the data.\n",
        "X, y = temporalize(X = input_X, y = input_y, lookback = lookback)\n",
        "\n",
        "print('For the same instance of y = 1, we are keeping past 5 samples in the 3D predictor array, X.')\n",
        "display(DataFrame(concatenate(X[where(array(y) == 1)[0][0]], axis=0))) "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First instance of y = 1 in the original data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>index</th>\n",
              "      <th>AAPL</th>\n",
              "      <th>INTC</th>\n",
              "      <th>NVDA</th>\n",
              "      <th>CSCO</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>MSFT</th>\n",
              "      <th>S</th>\n",
              "      <th>BAC</th>\n",
              "      <th>XLNX</th>\n",
              "      <th>WFC</th>\n",
              "      <th>^DJI</th>\n",
              "      <th>^GSPC</th>\n",
              "      <th>^NYA</th>\n",
              "      <th>^IXIC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.974611</td>\n",
              "      <td>0.992783</td>\n",
              "      <td>1.002580</td>\n",
              "      <td>1.010582</td>\n",
              "      <td>0.989890</td>\n",
              "      <td>0.959513</td>\n",
              "      <td>1.003400</td>\n",
              "      <td>0.991591</td>\n",
              "      <td>1.005288</td>\n",
              "      <td>1.003549</td>\n",
              "      <td>0.995074</td>\n",
              "      <td>1.000173</td>\n",
              "      <td>0.995838</td>\n",
              "      <td>0.996402</td>\n",
              "      <td>0.997792</td>\n",
              "      <td>0.996703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.002931</td>\n",
              "      <td>0.986314</td>\n",
              "      <td>1.064655</td>\n",
              "      <td>0.996306</td>\n",
              "      <td>1.018723</td>\n",
              "      <td>1.027301</td>\n",
              "      <td>1.072270</td>\n",
              "      <td>1.002580</td>\n",
              "      <td>0.985492</td>\n",
              "      <td>1.011313</td>\n",
              "      <td>0.988037</td>\n",
              "      <td>1.000862</td>\n",
              "      <td>1.006905</td>\n",
              "      <td>1.006525</td>\n",
              "      <td>1.006597</td>\n",
              "      <td>1.010156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "      <td>1.005578</td>\n",
              "      <td>1.013207</td>\n",
              "      <td>1.010717</td>\n",
              "      <td>0.988970</td>\n",
              "      <td>0.987810</td>\n",
              "      <td>0.998900</td>\n",
              "      <td>1.003168</td>\n",
              "      <td>0.998304</td>\n",
              "      <td>1.003197</td>\n",
              "      <td>0.998103</td>\n",
              "      <td>0.996237</td>\n",
              "      <td>0.997569</td>\n",
              "      <td>0.995440</td>\n",
              "      <td>0.997239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "      <td>1.005578</td>\n",
              "      <td>1.013207</td>\n",
              "      <td>1.010717</td>\n",
              "      <td>0.988970</td>\n",
              "      <td>0.987810</td>\n",
              "      <td>0.998900</td>\n",
              "      <td>1.003168</td>\n",
              "      <td>0.998304</td>\n",
              "      <td>1.003197</td>\n",
              "      <td>0.998103</td>\n",
              "      <td>0.996237</td>\n",
              "      <td>0.997569</td>\n",
              "      <td>0.995440</td>\n",
              "      <td>0.997239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.007129</td>\n",
              "      <td>1.003208</td>\n",
              "      <td>1.005578</td>\n",
              "      <td>1.013207</td>\n",
              "      <td>1.010717</td>\n",
              "      <td>0.988970</td>\n",
              "      <td>0.987810</td>\n",
              "      <td>0.998900</td>\n",
              "      <td>1.003168</td>\n",
              "      <td>0.998304</td>\n",
              "      <td>1.003197</td>\n",
              "      <td>0.998103</td>\n",
              "      <td>0.996237</td>\n",
              "      <td>0.997569</td>\n",
              "      <td>0.995440</td>\n",
              "      <td>0.997239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "      <td>1.022073</td>\n",
              "      <td>0.977006</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>0.991118</td>\n",
              "      <td>0.962158</td>\n",
              "      <td>0.983132</td>\n",
              "      <td>0.942736</td>\n",
              "      <td>0.994161</td>\n",
              "      <td>1.010526</td>\n",
              "      <td>1.004417</td>\n",
              "      <td>0.975026</td>\n",
              "      <td>1.000516</td>\n",
              "      <td>1.002356</td>\n",
              "      <td>1.000465</td>\n",
              "      <td>0.999541</td>\n",
              "      <td>0.994564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     y  index      AAPL      INTC  ...      ^DJI     ^GSPC      ^NYA     ^IXIC\n",
              "0  0.0      0  0.974611  0.992783  ...  0.995838  0.996402  0.997792  0.996703\n",
              "1  0.0      1  1.002931  0.986314  ...  1.006905  1.006525  1.006597  1.010156\n",
              "2  0.0      2  1.007129  1.003208  ...  0.996237  0.997569  0.995440  0.997239\n",
              "3  0.0      3  1.007129  1.003208  ...  0.996237  0.997569  0.995440  0.997239\n",
              "4  0.0      4  1.007129  1.003208  ...  0.996237  0.997569  0.995440  0.997239\n",
              "5  1.0      5  1.022073  0.977006  ...  1.002356  1.000465  0.999541  0.994564\n",
              "\n",
              "[6 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "For the same instance of y = 1, we are keeping past 5 samples in the 3D predictor array, X.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.990484</td>\n",
              "      <td>1.007319</td>\n",
              "      <td>1.018096</td>\n",
              "      <td>1.014062</td>\n",
              "      <td>1.008432</td>\n",
              "      <td>0.989583</td>\n",
              "      <td>0.981961</td>\n",
              "      <td>0.998545</td>\n",
              "      <td>1.001028</td>\n",
              "      <td>1.007470</td>\n",
              "      <td>0.997872</td>\n",
              "      <td>0.997619</td>\n",
              "      <td>1.002058</td>\n",
              "      <td>1.002425</td>\n",
              "      <td>1.002443</td>\n",
              "      <td>1.003806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>0.982265</td>\n",
              "      <td>0.970520</td>\n",
              "      <td>0.985500</td>\n",
              "      <td>0.969064</td>\n",
              "      <td>0.960632</td>\n",
              "      <td>0.968940</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.008763</td>\n",
              "      <td>0.995088</td>\n",
              "      <td>0.979694</td>\n",
              "      <td>0.998461</td>\n",
              "      <td>0.993005</td>\n",
              "      <td>0.992219</td>\n",
              "      <td>0.993652</td>\n",
              "      <td>0.989605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>0.982265</td>\n",
              "      <td>0.970520</td>\n",
              "      <td>0.985500</td>\n",
              "      <td>0.969064</td>\n",
              "      <td>0.960632</td>\n",
              "      <td>0.968940</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.008763</td>\n",
              "      <td>0.995088</td>\n",
              "      <td>0.979694</td>\n",
              "      <td>0.998461</td>\n",
              "      <td>0.993005</td>\n",
              "      <td>0.992219</td>\n",
              "      <td>0.993652</td>\n",
              "      <td>0.989605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.0</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>0.982265</td>\n",
              "      <td>0.970520</td>\n",
              "      <td>0.985500</td>\n",
              "      <td>0.969064</td>\n",
              "      <td>0.960632</td>\n",
              "      <td>0.968940</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.008763</td>\n",
              "      <td>0.995088</td>\n",
              "      <td>0.979694</td>\n",
              "      <td>0.998461</td>\n",
              "      <td>0.993005</td>\n",
              "      <td>0.992219</td>\n",
              "      <td>0.993652</td>\n",
              "      <td>0.989605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12.0</td>\n",
              "      <td>1.012328</td>\n",
              "      <td>0.994860</td>\n",
              "      <td>0.992038</td>\n",
              "      <td>0.979634</td>\n",
              "      <td>0.993913</td>\n",
              "      <td>0.993747</td>\n",
              "      <td>1.000684</td>\n",
              "      <td>1.000366</td>\n",
              "      <td>1.009231</td>\n",
              "      <td>1.005140</td>\n",
              "      <td>0.999636</td>\n",
              "      <td>1.006165</td>\n",
              "      <td>1.005132</td>\n",
              "      <td>1.004631</td>\n",
              "      <td>1.006186</td>\n",
              "      <td>1.000305</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0         1         2         3   ...        13        14        15        16\n",
              "0   8.0  0.990484  1.007319  1.018096  ...  1.002058  1.002425  1.002443  1.003806\n",
              "1   9.0  1.003529  0.982265  0.970520  ...  0.993005  0.992219  0.993652  0.989605\n",
              "2  10.0  1.003529  0.982265  0.970520  ...  0.993005  0.992219  0.993652  0.989605\n",
              "3  11.0  1.003529  0.982265  0.970520  ...  0.993005  0.992219  0.993652  0.989605\n",
              "4  12.0  1.012328  0.994860  0.992038  ...  1.005132  1.004631  1.006186  1.000305\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h89wHrdfyyyw",
        "colab_type": "text"
      },
      "source": [
        "The two tables are the same. This testifies that we are correctly taking 5 samples (= lookback), X(t):X(t-5) to predict y(t)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pmWNnDCyyyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(array(X), array(y), test_size=test_size, random_state=random_seed)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=test_size, random_state=random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3UzUymkyyy2",
        "colab_type": "code",
        "outputId": "384aa461-2911-409b-b7b0-347a9c858cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3404, 5, 1, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VV76ST8yyy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_y0 = X_train[y_train==0]\n",
        "X_train_y1 = X_train[y_train==1]\n",
        "X_valid_y0 = X_valid[y_valid==0]\n",
        "X_valid_y1 = X_valid[y_valid==1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzDCf3styyzB",
        "colab_type": "code",
        "outputId": "f0708467-0a79-43fb-fac3-3243a310636b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train_y0.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3351, 5, 1, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvqRBizyyzG",
        "colab_type": "text"
      },
      "source": [
        "#### Reshaping the data\n",
        "The tensors we have here are 4-dimensional. We will reshape them into the desired 3-dimensions corresponding to sample x lookback x features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-JtRrOqyyzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], lookback, n_features)\n",
        "X_train_y0 = X_train_y0.reshape(X_train_y0.shape[0], lookback, n_features)\n",
        "X_train_y1 = X_train_y1.reshape(X_train_y1.shape[0], lookback, n_features)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], lookback, n_features)\n",
        "\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], lookback, n_features)\n",
        "X_valid_y0 = X_valid_y0.reshape(X_valid_y0.shape[0], lookback, n_features)\n",
        "X_valid_y1 = X_valid_y1.reshape(X_valid_y1.shape[0], lookback, n_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzBW4HTNyyzL",
        "colab_type": "text"
      },
      "source": [
        "### Standardize the data\n",
        "It is usually better to use a standardized data (transformed to Gaussian, mean 0 and sd 1) for autoencoders.\n",
        "\n",
        "One common mistake is: we normalize the entire data and then split into train-test. This is not correct. Test data should be completely unseen to anything during the modeling. We should normalize the test data using the feature summary statistics computed from the training data. For normalization, these statistics are the mean and variance for each feature. \n",
        "\n",
        "The same logic should be used for the validation set. This makes the model more stable for a test data.\n",
        "\n",
        "To do this, we will require two UDFs.\n",
        "\n",
        "- `flatten`: This function will re-create the original 2D array from which the 3D arrays were created. This function is the inverse of `temporalize`, meaning `X = flatten(temporalize(X))`.\n",
        "- `scale`: This function will scale a 3D array that we created as inputs to the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRVsMJXAyyzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten(X):\n",
        "    '''\n",
        "    Flatten a 3D array.\n",
        "    Input        X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
        "    Output       flattened_X  A 2D array, sample x features.\n",
        "    '''\n",
        "    flattened_X = empty((X.shape[0], X.shape[2])) # Sample x features array.\n",
        "    for i in range(X.shape[0]):\n",
        "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
        "    return(flattened_X)\n",
        "\n",
        "def scale(X, scaler):\n",
        "    '''\n",
        "    Scale 3D array.\n",
        "    Inputs       X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
        "                 scaler       A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize\n",
        "    Output       X            Scaled 3D array.\n",
        "    '''\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
        "        \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqg4frnfyyzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a scaler using the training data.\n",
        "scaler = StandardScaler().fit(flatten(X_train_y0))\n",
        "X_train_y0_scaled = scale(X_train_y0, scaler)\n",
        "X_train_y1_scaled = scale(X_train_y1, scaler)\n",
        "X_train_scaled = scale(X_train, scaler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEY3ThchyyzZ",
        "colab_type": "code",
        "outputId": "45ae806c-a7ab-4346-cb08-9c85a89f9850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Test scaling validity.\n",
        "a = flatten(X_train_y0_scaled)\n",
        "print('Column-wise mean (should be all zeros):', mean(a, axis=0).round(6))\n",
        "print('Column-wise variance (should be all ones):', var(a, axis=0))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column-wise mean (should be all zeros): [ 0. -0.  0.  0.  0.  0. -0. -0.  0. -0. -0.  0. -0. -0.  0.  0.  0.]\n",
            "Column-wise variance (should be all ones): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xv9aimpyyzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale test and validation sets.\n",
        "X_valid_scaled = scale(X_valid, scaler)\n",
        "X_valid_y0_scaled = scale(X_valid_y0, scaler)\n",
        "X_test_scaled = scale(X_test, scaler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0o3SBX-yyzj",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Autoencoder training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vl3NZk_yyzj",
        "colab_type": "text"
      },
      "source": [
        "First we will initialize the Autoencoder architecture. We are building a simple autoencoder. More complex architectures and other configurations should be explored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXkULB_Hyyzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timesteps =  X_train_y0_scaled.shape[1] # Equal to the lookback.\n",
        "n_features =  X_train_y0_scaled.shape[2] # 59."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC2LW6CMyyzu",
        "colab_type": "code",
        "outputId": "09daecf3-db1a-4e41-83ec-b934d3d3a00e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "lstm_autoencoder = Sequential()\n",
        "# Encoder.\n",
        "lstm_autoencoder.add(LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
        "lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=False))\n",
        "lstm_autoencoder.add(RepeatVector(timesteps))\n",
        "# Decoder.\n",
        "lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=True))\n",
        "lstm_autoencoder.add(LSTM(32, activation='relu', return_sequences=True))\n",
        "lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
        "\n",
        "lstm_autoencoder.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 5, 32)             6400      \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 16)                3136      \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 5, 16)             0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 5, 16)             2112      \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 5, 32)             6272      \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 5, 17)             561       \n",
            "=================================================================\n",
            "Total params: 18,481\n",
            "Trainable params: 18,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eClUrTjiyyzy",
        "colab_type": "text"
      },
      "source": [
        "As a rule-of-thumb, look at the number of parameters. If not using any regularization, keep this less than the number of samples. If using regularization, depending on the degree of regularization you can let more parameters in the model that is greater than the sample size. For example, if using dropout with 0.5, you can have up to double the sample size (loosely speaking)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCGKhCicyyzz",
        "colab_type": "code",
        "outputId": "d2abbb99-8cac-40be-8ccc-3c4e85ec16c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Path to model weights (saved periodically).\n",
        "filepath = path + 'LSTM_autoencoder.h5'\n",
        "\n",
        "# Gradient descent optimization.\n",
        "optimizer = Adam(lr=learning_rate, clipnorm=1., clipvalue=0.5)\n",
        "\n",
        "# Training configuration.\n",
        "lstm_autoencoder.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Save model weights after each epoch if validation loss decreased.\n",
        "checkpointer = ModelCheckpoint(filepath=filepath, save_best_only=True, verbose=1)\n",
        "\n",
        "# Control learning rate schedule when validation is not improving.\n",
        "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, verbose=1, min_lr=learning_rate / 1000)\n",
        "\n",
        "# Various graphics.\n",
        "tbc = TensorBoardColab()\n",
        "\n",
        "# Shouldn't happen.\n",
        "term_on_NaN = TerminateOnNaN()\n",
        "\n",
        "lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, \n",
        "                                                X_train_y0_scaled, \n",
        "                                                epochs=epochs, \n",
        "                                                batch_size=batch_size, \n",
        "                                                validation_data=(X_valid_y0_scaled, \n",
        "                                                                 X_valid_y0_scaled), \n",
        "                                                callbacks=[checkpointer, \n",
        "                                                           reduce_lr, \n",
        "                                                           TensorBoardColabCallback(tbc), \n",
        "                                                           term_on_NaN], \n",
        "                                                verbose=1).history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://40d0e554.ngrok.io\n",
            "Train on 3351 samples, validate on 839 samples\n",
            "Epoch 1/200\n",
            "3351/3351 [==============================] - 5s 2ms/step - loss: 0.9946 - val_loss: 1.0769\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.07687, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 2/200\n",
            "3351/3351 [==============================] - 2s 574us/step - loss: 0.9821 - val_loss: 1.0478\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.07687 to 1.04784, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 3/200\n",
            "3351/3351 [==============================] - 2s 607us/step - loss: 0.9290 - val_loss: 0.9671\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.04784 to 0.96710, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 4/200\n",
            "3351/3351 [==============================] - 2s 598us/step - loss: 0.8707 - val_loss: 0.9316\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.96710 to 0.93157, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 5/200\n",
            "3351/3351 [==============================] - 2s 602us/step - loss: 0.8388 - val_loss: 0.8989\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.93157 to 0.89889, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 6/200\n",
            "3351/3351 [==============================] - 2s 591us/step - loss: 0.8050 - val_loss: 0.8689\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.89889 to 0.86889, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 7/200\n",
            "3351/3351 [==============================] - 2s 598us/step - loss: 0.7787 - val_loss: 0.8368\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.86889 to 0.83683, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 8/200\n",
            "3351/3351 [==============================] - 2s 599us/step - loss: 0.7554 - val_loss: 0.8238\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.83683 to 0.82376, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 9/200\n",
            "3351/3351 [==============================] - 2s 609us/step - loss: 0.7358 - val_loss: 0.7971\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.82376 to 0.79711, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 10/200\n",
            "3351/3351 [==============================] - 2s 608us/step - loss: 0.7178 - val_loss: 0.7825\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.79711 to 0.78249, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 11/200\n",
            "3351/3351 [==============================] - 2s 611us/step - loss: 0.7036 - val_loss: 0.7799\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.78249 to 0.77992, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 12/200\n",
            "3351/3351 [==============================] - 2s 599us/step - loss: 0.6888 - val_loss: 0.8002\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.77992\n",
            "Epoch 13/200\n",
            "3351/3351 [==============================] - 2s 597us/step - loss: 0.6761 - val_loss: 0.7472\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.77992 to 0.74715, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 14/200\n",
            "3351/3351 [==============================] - 2s 605us/step - loss: 0.6672 - val_loss: 0.7401\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.74715 to 0.74010, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 15/200\n",
            "3351/3351 [==============================] - 2s 605us/step - loss: 0.6585 - val_loss: 0.7256\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.74010 to 0.72563, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 16/200\n",
            "3351/3351 [==============================] - 2s 602us/step - loss: 0.6516 - val_loss: 0.7219\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.72563 to 0.72187, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 17/200\n",
            "3351/3351 [==============================] - 2s 607us/step - loss: 0.6454 - val_loss: 0.7192\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.72187 to 0.71917, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 18/200\n",
            "3351/3351 [==============================] - 2s 610us/step - loss: 0.6384 - val_loss: 0.7085\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.71917 to 0.70851, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 19/200\n",
            "3351/3351 [==============================] - 2s 615us/step - loss: 0.6346 - val_loss: 0.7238\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.70851\n",
            "Epoch 20/200\n",
            "3351/3351 [==============================] - 2s 607us/step - loss: 0.6280 - val_loss: 0.7079\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.70851 to 0.70793, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 21/200\n",
            "3351/3351 [==============================] - 2s 591us/step - loss: 0.6227 - val_loss: 0.6963\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.70793 to 0.69627, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 22/200\n",
            "3351/3351 [==============================] - 2s 584us/step - loss: 0.6183 - val_loss: 0.6972\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69627\n",
            "Epoch 23/200\n",
            "3351/3351 [==============================] - 2s 603us/step - loss: 0.6123 - val_loss: 0.6955\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.69627 to 0.69553, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 24/200\n",
            "3351/3351 [==============================] - 2s 619us/step - loss: 0.6079 - val_loss: 0.6935\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.69553 to 0.69347, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 25/200\n",
            "3351/3351 [==============================] - 2s 605us/step - loss: 0.6019 - val_loss: 0.6863\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.69347 to 0.68634, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 26/200\n",
            "3351/3351 [==============================] - 2s 594us/step - loss: 0.5979 - val_loss: 0.6740\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.68634 to 0.67396, saving model to /content/gdrive/My Drive/LSTM_autoencoder/LSTM_autoencoder.h5\n",
            "Epoch 27/200\n",
            "2368/3351 [====================>.........] - ETA: 0s - loss: 0.5831"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxtGGYuHyyz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(lstm_autoencoder_history['loss'], linewidth=2, label='Train')\n",
        "plt.plot(lstm_autoencoder_history['val_loss'], linewidth=2, label='Valid')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OScEKx1Pyyz7",
        "colab_type": "text"
      },
      "source": [
        "### Sanity check\n",
        "Doing a sanity check by validating the reconstruction error \n",
        "on the train data. Here we will reconstruct the entire train \n",
        "data with both 0 and 1 labels.\n",
        "\n",
        "**Expectation**: the reconstruction error of 0 labeled data should\n",
        "be smaller than 1.\n",
        "\n",
        "**Caution**: do not use this result for model evaluation. It may\n",
        "result into overfitting issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbAw7kQDyyz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x_predictions = lstm_autoencoder.predict(X_train_scaled)\n",
        "mse = mean(power(flatten(X_train_scaled) - flatten(train_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = DataFrame({'Reconstruction_error': mse,\n",
        "                       'True_class': y_train.tolist()})\n",
        "\n",
        "groups = error_df.groupby('True_class')\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "            label = 'Break' if name == 1 else 'Normal')\n",
        "ax.legend()\n",
        "plt.title('Reconstruction error for different classes')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.xlabel('Data point index')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayrKThIyyy0A",
        "colab_type": "text"
      },
      "source": [
        "## Predictions using the Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZvmE6JIyy0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_x_predictions = lstm_autoencoder.predict(X_valid_scaled)\n",
        "mse = mean(power(flatten(X_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = DataFrame({'Reconstruction_error': mse,\n",
        "                        'True_class': y_valid.tolist()})\n",
        "\n",
        "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\n",
        "plt.plot(threshold_rt, precision_rt[1:], label='Precision', linewidth=5)\n",
        "plt.plot(threshold_rt, recall_rt[1:], label='Recall', linewidth=5)\n",
        "plt.title('Precision and recall for different threshold values')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Precision/Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oowOhqFXyy0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x_predictions = lstm_autoencoder.predict(X_test_scaled)\n",
        "mse = mean(power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = DataFrame({'Reconstruction_error': mse,\n",
        "                      'True_class': y_test.tolist()})\n",
        "\n",
        "threshold_fixed = 0.3\n",
        "groups = error_df.groupby('True_class')\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "            label= 'Break' if name == 1 else 'Normal')\n",
        "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors='r', zorder=100, label='Threshold')\n",
        "ax.legend()\n",
        "plt.title('Reconstruction error for different classes')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.xlabel('Data point index')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UzmkPL0yy0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYZpiO7Xyy0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='d');\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlMlT2E2yy0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)\n",
        "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
        "\n",
        "plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n",
        "plt.plot([0,1],[0,1], linewidth=5)\n",
        "\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}