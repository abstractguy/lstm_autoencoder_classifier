{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "LSTM_autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abstractguy/lstm_autoencoder_classifier/blob/master/LSTM_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZOZmPmIyyxr",
        "colab_type": "text"
      },
      "source": [
        "**Dataset: LSTM Autoencoder for Rare Event Binary Classification in Multivariate Time Series**\n",
        "\n",
        "https://arxiv.org/abs/1809.10717 (please cite this article, if using the dataset).\n",
        "\n",
        "https://towardsdatascience.com/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098\n",
        "\n",
        "https://github.com/cran2367/autoencoder_classifier/blob/master/autoencoder_classifier.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MggK-vZ9ETZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Money maker by Samuel Duclos.\n",
        "AI = 'LSTM_autoencoder'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZKnnbYDEUti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hardcode parameters.\n",
        "if AI == 'LSTM':\n",
        "    main_ticker = 'AAPL'\n",
        "    trends = False\n",
        "    batch_size = 64\n",
        "    epochs = 100\n",
        "    learning_rate = 0.000003\n",
        "    test_size = 0.2\n",
        "    lookback = 5 # Days of past data.\n",
        "    tickers_list = []\n",
        "\n",
        "elif AI == 'LSTM_autoencoder':\n",
        "    main_ticker = 'FB'\n",
        "    trends = True\n",
        "    batch_size = 32\n",
        "    epochs = 100\n",
        "    learning_rate = 0.0001\n",
        "    random_seed = 123\n",
        "    test_size = 0.15\n",
        "    lookback = 5 # Days of past data.\n",
        "    gain = 0.055 # Percentage of gain to consider good for trade.\n",
        "    tickers_list = ['GE', \n",
        "                    'ADS', \n",
        "                    'INTC', \n",
        "                    'AAPL', \n",
        "                    'NVDA', \n",
        "                    'CSCO', \n",
        "                    'AMD', \n",
        "                    'AMZN', \n",
        "                    'GOOG', \n",
        "                    'MSFT', \n",
        "                    'S', \n",
        "                    'BAC', \n",
        "                    'XLNX', \n",
        "                    'WFC', \n",
        "                    '^DJI', \n",
        "                    '^GSPC', \n",
        "                    '^NYA', \n",
        "                    '^IXIC']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIaFe10aEU8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytrends"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJuuvLMCEVuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from collections import OrderedDict\n",
        "from math import sqrt\n",
        "from os import chdir\n",
        "from os.path import exists\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from numpy.random import seed\n",
        "from numpy import append, array, concatenate, count_nonzero, empty, empty_like, expand_dims, mean, nan, power, var, where, zeros\n",
        "from pandas import bdate_range, concat, DataFrame, date_range, read_csv, Series\n",
        "from pandas_datareader.data import DataReader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import auc, classification_report, confusion_matrix, f1_score, mean_squared_error, precision_recall_curve, precision_recall_fscore_support, recall_score, roc_curve\n",
        "from tensorflow import set_random_seed\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import load_model, Model, Sequential\n",
        "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras import optimizers, Sequential\n",
        "from google.colab.drive import mount\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "from pytrends.request import TrendReq\n",
        "from pylab import rcParams\n",
        "from seaborn import heatmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed(7)\n",
        "set_random_seed(11)\n",
        "rcParams['figure.figsize'] = 8, 6\n",
        "labels = ['Normal', 'Break']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9PfXIoazLPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/gdrive/'\n",
        "mount(path)\n",
        "path = path + 'My Drive/LSTM_autoencoder/'\n",
        "chdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koUXcR8eCYt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_ticker_to_tickers(symbol, ticker, tickers, join='inner'):\n",
        "    tickers = tickers[tickers.index >= ticker.index.min()]\n",
        "\n",
        "    if symbol not in tickers.columns:\n",
        "        return concat([tickers, ticker.to_frame(name=symbol)], axis='columns', join=join)\n",
        "    else:\n",
        "        return tickers\n",
        "\n",
        "'''\n",
        "def add_ticker_to_tickers_old(symbol, ticker, tickers):\n",
        "    minimum_date = ticker.index.min()\n",
        "    overall_minimum_date = tickers.index.min()\n",
        "    if minimum_date > overall_minimum_date:\n",
        "        start = overall_minimum_date\n",
        "        tickers = tickers[tickers.index >= minimum_date]\n",
        "\n",
        "    tickers[symbol][tickers.index.isin(ticker.index)] = ticker\n",
        "\n",
        "    return tickers\n",
        "'''\n",
        "\n",
        "def download_ticker_from_yahoo(symbol, start=None, end=None):\n",
        "    if start is None or end is None:\n",
        "        start = datetime(1970, 1, 1)\n",
        "        end = datetime.now()\n",
        "\n",
        "    return DataReader(symbol, 'yahoo', start=start, end=end)\n",
        "\n",
        "def download_trends_from_google(symbol, start=None, end=None):\n",
        "    if start is None or end is None:\n",
        "        start = datetime(1970, 1, 1)\n",
        "        end = datetime.now()\n",
        "\n",
        "    pytrends.build_payload([symbol], timeframe='today 5-y')\n",
        "    return pytrends.interest_over_time()[symbol]\n",
        "\n",
        "def get_stock(main_ticker, tickers_list, path=path, trends=True):\n",
        "    csv_path = path + 'dataset.csv'\n",
        "\n",
        "    if exists(csv_path):\n",
        "        tickers = read_csv(csv_path)\n",
        "    else:\n",
        "        start = datetime(1970, 1, 1)\n",
        "        end = datetime.now()\n",
        "        tickers = DataReader(main_ticker, 'yahoo', start=start, end=end).High.dropna().to_frame(name=main_ticker)\n",
        "\n",
        "        for symbol in tqdm(tickers_list, unit='symbol'):\n",
        "            try:\n",
        "                #ticker = DataReader(symbol, 'yahoo', start=start, end=end)\n",
        "                ticker = download_ticker_from_yahoo(symbol, start=start, end=end)\n",
        "                tickers = add_ticker_to_tickers(symbol, ticker.High, tickers, join='inner')\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        business_days = (~tickers.dropna().asfreq('D').isna().any(axis='columns')).astype(int)\n",
        "\n",
        "        if trends:\n",
        "            pytrends = TrendReq(hl='en-US', tz=360)\n",
        "            for symbol in tqdm([main_ticker] + tickers_list, unit='symbol'):\n",
        "                try:\n",
        "                    #pytrends.build_payload([symbol], timeframe='today 5-y')\n",
        "                    #ticker = pytrends.interest_over_time()[symbol]\n",
        "                    ticker = download_trends_from_google(symbol, start=start, end=end)\n",
        "                    tickers = add_ticker_to_tickers(symbol + '_trend', ticker, tickers, join='outer')\n",
        "\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        ticker = DataReader(main_ticker, 'yahoo', start=start, end=end)\n",
        "        ticker_y = ticker.Close / ticker.Open\n",
        "        tickers = add_ticker_to_tickers(main_ticker + '_y', ticker_y, tickers, join='inner')\n",
        "\n",
        "        tickers = tickers.asfreq('D').fillna(method='backfill')\n",
        "        tickers = add_ticker_to_tickers('Business_days', business_days, tickers, join='inner')\n",
        "\n",
        "        tickers.to_csv(csv_path)\n",
        "\n",
        "    return tickers\n",
        "\n",
        "def extract_column(dataset, column_name):\n",
        "    extracted = dataset[column_name].copy()\n",
        "    dataset = dataset.drop(columns=[column_name])\n",
        "    return dataset, extracted\n",
        "\n",
        "def delta_time_series(data):\n",
        "    data = data.astype('float32').values\n",
        "    return data[1:] - data[:-1]\n",
        "\n",
        "def plot_dataset(dataset):\n",
        "    plt.plot(dataset)\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Derivatives')\n",
        "    plt.show()\n",
        "\n",
        "def get_y_from_generator(generator):\n",
        "    '''Get all targets y from a TimeseriesGenerator instance.'''\n",
        "    y = None\n",
        "\n",
        "    for i in range(len(generator)):\n",
        "        batch_y = generator[i][1]\n",
        "\n",
        "        if y is None:\n",
        "            y = batch_y\n",
        "        else:\n",
        "            y = append(y, batch_y)\n",
        "\n",
        "    y = y.reshape((-1, 1))\n",
        "    print(y.shape)\n",
        "    return y\n",
        "\n",
        "def binary_accuracy(a, b, name='training'):\n",
        "    '''Helper function to compute the match score of two binary numpy arrays.'''\n",
        "    a = a[:,0] > 0\n",
        "    b = b[:,0] > 0\n",
        "    assert len(a) == len(b)\n",
        "    print('Binary accuracy (' + name + ' data):', (a == b).sum() / len(a))\n",
        "\n",
        "sign = lambda x: (1, -1)[x < 0]\n",
        "\n",
        "def curve_shift(dataset, shift_by):\n",
        "    vector = dataset.y.copy()\n",
        "    for s in range(abs(shift_by)):\n",
        "        vector += vector.shift(sign(shift_by), fill_value=0)\n",
        "\n",
        "    dataset['ytmp'] = vector\n",
        "    dataset = dataset.drop(dataset[dataset['y'] == 1].index)\n",
        "    dataset = dataset.drop('y', axis=1).rename(columns={'ytmp': 'y'})\n",
        "    dataset['y'] = (dataset.y > 0).astype(int)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def temporalize(X, y, lookback):\n",
        "    output_X = []\n",
        "    output_y = []\n",
        "    for i in range(len(X) - lookback - 1):\n",
        "        t = []\n",
        "        for j in range(1, lookback + 1):\n",
        "            t.append(X[[(i + j + 1)],:]) # Gather past records up to lookback.\n",
        "\n",
        "        output_X.append(t)\n",
        "        output_y.append(y[i + lookback + 1])\n",
        "\n",
        "    return output_X, output_y\n",
        "\n",
        "# 3D -> 2D.\n",
        "# (samples, timesteps, features) -> (samples, features).\n",
        "def flatten(X):\n",
        "    flattened_X = empty((X.shape[0], X.shape[2])) # Sample X feature array.\n",
        "    for i in range(X.shape[0]):\n",
        "        flattened_X[i] = X[i,X.shape[1] - 1,:]\n",
        "\n",
        "    return(flattened_X)\n",
        "\n",
        "# Scale samples individually.\n",
        "# (samples, timesteps, features) -> (samples, timesteps, features).\n",
        "def scale(X, scaler):\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:,:] = scaler.transform(X[i,:,:])\n",
        "\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ1ce3K4Cfbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_csv = get_stock(main_ticker, tickers_list, trends=trends)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5auDgZuAGn5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset, ticker_y = extract_column(dataset_csv, main_ticker + '_y')\n",
        "dataset, business_days = extract_column(dataset, 'Business_days')\n",
        "yesterday_date = dataset.index[-1]\n",
        "yesterday_score = dataset.iloc[-1,dataset.columns == main_ticker]\n",
        "dataset['Business_days'] = business_days\n",
        "plot_dataset(dataset)\n",
        "dataset.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCdAtlPssP-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_dataset(ticker_y)\n",
        "ticker_y.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ij_2mhC9hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ground truth.\n",
        "dataset['y'] = (ticker_y > (1.0 + (gain / 2.0))).astype(int)\n",
        "print('Percentage of ones (keep less than 5%):', \n",
        "      count_nonzero(dataset.y) / ticker_y.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aMNeaIpn48n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO_8_S2myyyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Before shifting.')\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi2t-uXVcfgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('After shifting.')\n",
        "dataset = curve_shift(dataset, shift_by=-1)\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ainlXJUFjrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc41Mo3qSRXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.drop(columns=['Date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rosttrXFyyyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the DataFrame to a numpy array.\n",
        "input_X, input_y = extract_column(dataset, 'y')\n",
        "input_X = input_X.values\n",
        "input_y = input_y.values\n",
        "\n",
        "# Number of features.\n",
        "n_features = input_X.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgLp2Oyiyyyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test: The 3D tensors (arrays) for LSTM are forming correctly.\n",
        "print('First instance of y = 1 in the original data.')\n",
        "display(dataset.iloc[(where(array(input_y) == 1)[0][0] - lookback):(where(array(input_y) == 1)[0][0] + 1),])\n",
        "\n",
        "# Temporalize the data.\n",
        "X, y = temporalize(X=input_X, y=input_y, lookback=lookback)\n",
        "\n",
        "print('For the same instance of y=1, we are keeping past 5 samples in the 3D predictor array, X.')\n",
        "display(DataFrame(concatenate(X[where(array(y) == 1)[0][0]], axis=0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h89wHrdfyyyw",
        "colab_type": "text"
      },
      "source": [
        "The two tables are the same. This testifies that we are correctly taking 5 samples (= lookback), X(t):X(t-5) to predict y(t)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pmWNnDCyyyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(array(X), array(y), test_size=test_size, random_state=random_seed)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=test_size, random_state=random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VV76ST8yyy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_y0 = X_train[y_train==0]\n",
        "X_train_y1 = X_train[y_train==1]\n",
        "X_valid_y0 = X_valid[y_valid==0]\n",
        "X_valid_y1 = X_valid[y_valid==1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3UzUymkyyy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzDCf3styyzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_y0.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-JtRrOqyyzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (sample, channel, lookback, feature) -> (sample, lookback, feature).\n",
        "X_train = X_train.reshape(X_train.shape[0], lookback, n_features)\n",
        "X_test = X_test.reshape(X_test.shape[0], lookback, n_features)\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], lookback, n_features)\n",
        "\n",
        "X_train_y0 = X_train_y0.reshape(X_train_y0.shape[0], lookback, n_features)\n",
        "X_train_y1 = X_train_y1.reshape(X_train_y1.shape[0], lookback, n_features)\n",
        "\n",
        "X_valid_y0 = X_valid_y0.reshape(X_valid_y0.shape[0], lookback, n_features)\n",
        "X_valid_y1 = X_valid_y1.reshape(X_valid_y1.shape[0], lookback, n_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqg4frnfyyzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a scaler using the training data.\n",
        "scaler = StandardScaler().fit(flatten(X_train_y0))\n",
        "X_train_y0_scaled = scale(X_train_y0, scaler)\n",
        "X_train_y1_scaled = scale(X_train_y1, scaler)\n",
        "X_train_scaled = scale(X_train, scaler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEY3ThchyyzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test scaling validity.\n",
        "a = flatten(X_train_y0_scaled)\n",
        "print('Column-wise mean (should be all zeros):', mean(a, axis=0).round(6))\n",
        "print('Column-wise variance (should be all ones):', var(a, axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xv9aimpyyzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale test and validation sets.\n",
        "X_valid_scaled = scale(X_valid, scaler)\n",
        "X_valid_y0_scaled = scale(X_valid_y0, scaler)\n",
        "X_test_scaled = scale(X_test, scaler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXkULB_Hyyzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, timesteps, n_features = X_train_y0_scaled.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC2LW6CMyyzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_autoencoder = Sequential()\n",
        "# Encoder.\n",
        "LSTM_autoencoder.add(LSTM(units=32, \n",
        "                          activation='relu', \n",
        "                          input_shape=(timesteps, n_features), \n",
        "                          return_sequences=True))\n",
        "\n",
        "LSTM_autoencoder.add(LSTM(units=16, activation='relu', return_sequences=False))\n",
        "LSTM_autoencoder.add(RepeatVector(timesteps))\n",
        "# Decoder.\n",
        "LSTM_autoencoder.add(LSTM(units=16, activation='relu', return_sequences=True))\n",
        "LSTM_autoencoder.add(LSTM(units=32, activation='relu', return_sequences=True))\n",
        "LSTM_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
        "\n",
        "# Keep parameters less than non-droped out features.\n",
        "LSTM_autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCGKhCicyyzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM autoencoder model for rare stock event prediction.\n",
        "# Path to model weights (saved periodically).\n",
        "filepath = path + 'LSTM_autoencoder.h5'\n",
        "if exists(filepath):\n",
        "    LSTM_autoencoder = load_model(filepath)\n",
        "else:\n",
        "    # Gradient descent optimization.\n",
        "    optimizer = Adam(lr=learning_rate, clipnorm=1., clipvalue=0.5)\n",
        "\n",
        "    # Training configuration.\n",
        "    LSTM_autoencoder.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "    # Save model weights after each epoch if validation loss decreased.\n",
        "    checkpointer = ModelCheckpoint(filepath=filepath, \n",
        "                                   save_best_only=True, \n",
        "                                   verbose=1)\n",
        "\n",
        "    # Control learning rate schedule when validation is not improving.\n",
        "    reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
        "                                  patience=epochs // 25, \n",
        "                                  verbose=1, \n",
        "                                  min_lr=learning_rate / 1000)\n",
        "\n",
        "    # Various graphics.\n",
        "    tbc = TensorBoardColab()\n",
        "\n",
        "    # Shouldn't happen.\n",
        "    term_on_NaN = TerminateOnNaN()\n",
        "\n",
        "    history = LSTM_autoencoder.fit(X_train_y0_scaled, \n",
        "                                   X_train_y0_scaled, \n",
        "                                   epochs=epochs, \n",
        "                                   batch_size=batch_size, \n",
        "                                   validation_data=(X_valid_y0_scaled, \n",
        "                                                    X_valid_y0_scaled), \n",
        "                                   callbacks=[checkpointer, \n",
        "                                              reduce_lr, \n",
        "                                              TensorBoardColabCallback(tbc), \n",
        "                                              term_on_NaN], \n",
        "                                              verbose=1).history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw-O3G2nLjXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print training and test loss histories.\n",
        "try:\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "\n",
        "# Skip because model was probably loaded from file.\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbAw7kQDyyz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Reconstruction error should be less than one.')\n",
        "\n",
        "train_x_predictions = LSTM_autoencoder.predict(X_train_scaled)\n",
        "mse = mean(power(flatten(X_train_scaled) - flatten(train_x_predictions), 2), \n",
        "           axis=1)\n",
        "\n",
        "error = DataFrame({'Reconstruction_error': mse, 'True_class': y_train.tolist()})\n",
        "\n",
        "groups = error.groupby('True_class')\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, \n",
        "            group.Reconstruction_error, \n",
        "            marker='o', \n",
        "            ms=3.5, \n",
        "            linestyle='',\n",
        "            label = 'Break' if name == 1 else 'Normal')\n",
        "\n",
        "ax.legend()\n",
        "plt.title('Reconstruction error for different classes')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.xlabel('Data point index')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZvmE6JIyy0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions on validation data.\n",
        "valid_x_predictions = LSTM_autoencoder.predict(X_valid_scaled)\n",
        "mse = mean(power(flatten(X_valid_scaled) - flatten(valid_x_predictions), 2), \n",
        "           axis=1)\n",
        "\n",
        "error = DataFrame({'Reconstruction_error': mse, 'True_class': y_valid.tolist()})\n",
        "\n",
        "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error.True_class, \n",
        "                                                               error.Reconstruction_error)\n",
        "\n",
        "plt.plot(threshold_rt, precision_rt[1:], label='Precision', linewidth=5)\n",
        "plt.plot(threshold_rt, recall_rt[1:], label='Recall', linewidth=5)\n",
        "plt.title('Precision and recall for different threshold values')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Precision/Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oowOhqFXyy0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions on testing data.\n",
        "test_x_predictions = LSTM_autoencoder.predict(X_test_scaled)\n",
        "mse = mean(power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), \n",
        "           axis=1)\n",
        "\n",
        "error = DataFrame({'Reconstruction_error': mse, 'True_class': y_test.tolist()})\n",
        "\n",
        "threshold_fixed = 0.3\n",
        "groups = error.groupby('True_class')\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, \n",
        "            group.Reconstruction_error, \n",
        "            marker='o', \n",
        "            ms=3.5, \n",
        "            linestyle='',\n",
        "            label= 'Break' if name == 1 else 'Normal')\n",
        "\n",
        "ax.hlines(threshold_fixed, \n",
        "          ax.get_xlim()[0], \n",
        "          ax.get_xlim()[1], \n",
        "          colors='r', \n",
        "          zorder=100, \n",
        "          label='Threshold')\n",
        "\n",
        "ax.legend()\n",
        "plt.title('Reconstruction error for different classes')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.xlabel('Data point index')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UzmkPL0yy0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_y = [1 if e > threshold_fixed else 0 for e in error.Reconstruction_error.values]\n",
        "conf_matrix = confusion_matrix(error.True_class, pred_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYZpiO7Xyy0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "heatmap(conf_matrix, \n",
        "        xticklabels=labels, \n",
        "        yticklabels=labels, \n",
        "        annot=True, \n",
        "        fmt='d')\n",
        "\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js7P_pZZmu_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_pos_rate, true_pos_rate, thresholds = roc_curve(error.True_class, \n",
        "                                                      error.Reconstruction_error)\n",
        "roc_auc = auc(false_pos_rate, true_pos_rate,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlMlT2E2yy0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(false_pos_rate, \n",
        "         true_pos_rate, \n",
        "         linewidth=5, \n",
        "         label='AUC = ' + str(round(roc_auc, 3)))\n",
        "plt.plot([0, 1], [0, 1], linewidth=5)\n",
        "\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}